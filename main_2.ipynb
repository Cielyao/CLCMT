{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "from model.td3 import TD3\n",
    "from env.LaneChange_v2 import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import utils\n",
    "\n",
    "\n",
    "# Runs policy for X episodes and returns average reward\n",
    "# A fixed seed is used for the eval environment\n",
    "def eval_policy(policy, args, env, seed, eval_episodes=10):\n",
    "    eval_env = env\n",
    "    eval_env.seed(seed + 100)\n",
    "    episode_timesteps = 0\n",
    "\n",
    "    avg_reward = 0\n",
    "    avg_reward0 = 0\n",
    "    avg_reward1 = 0\n",
    "    avg_reward2 = 0\n",
    "    avg_reward3 = 0\n",
    "    avg_reward4 = 0\n",
    "    # avg_reward5 = 0\n",
    "    num_steps = 0\n",
    "    num_bingo = 0\n",
    "    num_crash = 0\n",
    "    num_end = 0\n",
    "    # num_fail = 0\n",
    "    # num_warn = 0\n",
    "    trajectory_x = []\n",
    "    trajectory_y = []\n",
    "\n",
    "    for i in range(eval_episodes):\n",
    "        state, done, info = eval_env.reset()\n",
    "        episode_timesteps = 0\n",
    "        # \t\teval_env.render()\n",
    "        if i == 0:\n",
    "            trajectory_x.append(eval_env.obj_veh.x)\n",
    "            trajectory_y.append(eval_env.obj_veh.y)\n",
    "        while not done and episode_timesteps != args.episode_max_iter:\n",
    "            episode_timesteps += 1\n",
    "            action = policy.select_action(np.array(state))\n",
    "            state, reward, done, info = eval_env.step(action)\n",
    "            # \t\t\teval_env.render()\n",
    "            avg_reward += reward\n",
    "            num_steps += 1\n",
    "            num_crash += info['crash']\n",
    "            num_bingo += info['bingo']\n",
    "            # num_fail += info['fail']\n",
    "            # num_warn += info['warn']\n",
    "            num_end += info['end']\n",
    "        avg_reward0 = info['reward'][0]\n",
    "        avg_reward1 = info['reward'][1]\n",
    "        avg_reward2 = info['reward'][2]\n",
    "        avg_reward3 = info['reward'][3]\n",
    "        avg_reward4 = info['reward'][4]\n",
    "        # avg_reward5 = info['reward'][5]\n",
    "        if i == 0:\n",
    "            trajectory_x.append(eval_env.obj_veh.x)\n",
    "            trajectory_y.append(eval_env.obj_veh.y)\n",
    "\n",
    "\n",
    "    avg_reward = avg_reward / eval_episodes\n",
    "    avg_reward0 = avg_reward0 / eval_episodes\n",
    "    avg_reward1 = avg_reward1 / eval_episodes\n",
    "    avg_reward2 = avg_reward2 / eval_episodes\n",
    "    avg_reward3 = avg_reward3 / eval_episodes\n",
    "    avg_reward4 = avg_reward4 / eval_episodes\n",
    "    # avg_reward5 = avg_reward5 / eval_episodes\n",
    "    num_steps = num_steps / eval_episodes\n",
    "    num_crash = num_crash / eval_episodes\n",
    "    num_bingo = num_bingo / eval_episodes\n",
    "    num_end = num_end / eval_episodes\n",
    "    # num_fail = num_fail / eval_episodes\n",
    "    # num_warn = num_warn / eval_episodes\n",
    "\n",
    "    # eval_env.render(close=True)\n",
    "    # print(\"---------------------------------------\")\n",
    "    print(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}\")\n",
    "    print(\"---------------------------------------\")\n",
    "    return avg_reward, num_steps, num_crash, num_bingo, num_end, avg_reward0, avg_reward1, avg_reward2, avg_reward3, avg_reward4, [\n",
    "        trajectory_x, trajectory_y]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--policy\", default=\"TD3\")  # Policy name (TD3, DDPG or OurDDPG)\n",
    "    parser.add_argument(\"--env\", default=\"LaneChangeEnv\")  # OpenAI gym environment name\n",
    "    parser.add_argument(\"--seed\", default=2, type=int)  # Sets Gym, PyTorch and Numpy seeds\n",
    "    parser.add_argument(\"--start_timesteps\", default=25e3, type=int)  # Time steps initial random policy is used\n",
    "    parser.add_argument(\"--eval_freq\", default=5e3, type=int)  # How often (time steps) we evaluate 5e3\n",
    "    parser.add_argument(\"--max_timesteps\", default=6e5, type=int)  # Max time steps to run environment\n",
    "    parser.add_argument(\"--expl_noise\", default=0.5)  # Std of Gaussian exploration noise\n",
    "    parser.add_argument(\"--batch_size\", default=256, type=int)  # Batch size for both actor and critic\n",
    "    parser.add_argument(\"--discount\", default=0.99)  # Discount factor\n",
    "    parser.add_argument(\"--tau\", default=0.005)  # Target network update rate\n",
    "    parser.add_argument(\"--policy_noise\", default=0.5)  # Noise added to target policy during critic update\n",
    "    parser.add_argument(\"--noise_clip\", default=0.5)  # Range to clip target policy noise\n",
    "    parser.add_argument(\"--policy_freq\", default=2, type=int)  # Frequency of delayed policy updates\n",
    "    parser.add_argument(\"--save_model\", action=\"store_true\")  # Save model and optimizer parameters\n",
    "    parser.add_argument(\"--load_model\", default=\"\")  # Model load file name, \"\" doesn't load, \"default\" uses file_name\n",
    "    parser.add_argument(\"--episode_max_iter\", default=500)\n",
    "    # \tparser.add_argument(\"--case_name\", default=\"CC_space30_1\")\n",
    "    parser.add_argument(\"--case_name\", default=\"CC_test\")\n",
    "\n",
    "    #     \targs = parser.parse_args()\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    file_name = f\"{args.policy}_{args.env}_{args.seed}_{args.case_name}\"\n",
    "    print(\"---------------------------------------\")\n",
    "    print(f\"Policy: {args.policy}, Env: {args.env}, Seed: {args.seed}\")\n",
    "    print(\"---------------------------------------\")\n",
    "\n",
    "    if not os.path.exists(\"./results\"):\n",
    "        os.makedirs(\"./results\")\n",
    "\n",
    "    if args.save_model and not os.path.exists(\"./models\"):\n",
    "        os.makedirs(\"./models\")\n",
    "\n",
    "    env = eval(args.env)()\n",
    "\n",
    "    # \tSet seeds\n",
    "    env.seed(args.seed)\n",
    "    env.action_space.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    max_action = float(env.action_space.high[0])\n",
    "\n",
    "    kwargs = {\n",
    "        \"state_dim\": state_dim,\n",
    "        \"action_dim\": action_dim,\n",
    "        \"max_action\": max_action,\n",
    "        \"discount\": args.discount,\n",
    "        \"tau\": args.tau,\n",
    "    }\n",
    "\n",
    "    # Initialize policy\n",
    "    if args.policy == \"TD3\":\n",
    "        # Target policy smoothing is scaled wrt the action scale\n",
    "        kwargs[\"policy_noise\"] = args.policy_noise * max_action\n",
    "        kwargs[\"noise_clip\"] = args.noise_clip * max_action\n",
    "        kwargs[\"policy_freq\"] = args.policy_freq\n",
    "        policy = TD3(**kwargs)\n",
    "    #elif args.policy == \"OurDDPG\":\n",
    "    #policy = OurDDPG.DDPG(**kwargs)\n",
    "    #elif args.policy == \"DDPG\":\n",
    "    #policy = DDPG.DDPG(**kwargs)\n",
    "\n",
    "    if args.load_model != \"\":\n",
    "        policy_file = file_name if args.load_model == \"default\" else args.load_model\n",
    "        policy.load(f\"./models/{policy_file}\")\n",
    "\n",
    "    replay_buffer = utils.ReplayBuffer(state_dim, action_dim)\n",
    "    # Evaluate untrained policy\n",
    "    evaluations = [eval_policy(policy, args, copy.deepcopy(env), args.seed)]\n",
    "\n",
    "    state, done, info = env.reset()\n",
    "    # \tenv.render()\n",
    "    episode_reward = 0\n",
    "    episode_timesteps = 0\n",
    "    episode_num = 0\n",
    "    # \tloss = []\n",
    "    # \titeration =[]\n",
    "    # \tloss_ac = []\n",
    "    # \titeration_ac=[]\n",
    "    for t in range(int(args.max_timesteps)):\n",
    "        episode_timesteps += 1\n",
    "\n",
    "        # Select action randomly or according to policy\n",
    "        if t < args.start_timesteps:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # hsr added: could add decay exploration rate to control exploration and exploitation.\n",
    "            # 2. sparse reward probelm also could add intrisic reward to encourage exploration, to  be continue.\n",
    "            action = (\n",
    "                    policy.select_action(np.array(state))\n",
    "                    + np.random.normal(0, max_action * args.expl_noise, size=action_dim)\n",
    "            ).clip(-max_action, max_action)\n",
    "\n",
    "        # Perform action\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        # \t\tenv.render()\n",
    "        done_bool = float(done) if episode_timesteps < env._max_episode_steps else 0\n",
    "\n",
    "        # Store data in replay buffer\n",
    "        replay_buffer.add(state, action, next_state, reward, done_bool)\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Train agent after collecting sufficient data\n",
    "        if t >= args.start_timesteps:\n",
    "            policy.train(replay_buffer, args.batch_size)\n",
    "        #env.render()\n",
    "\n",
    "        # \t\t\tcritic_loss = policy.train(replay_buffer, args.batch_size)\n",
    "        # \t\t\tcritic_loss,actor_loss = policy.train(replay_buffer, args.batch_size)\n",
    "        # # \t\t\tprint(actor_loss,t)\n",
    "        # \t\t\titeration.append(t)\t\t#i是你的iter\n",
    "        # \t\t\titeration_ac.append(t)\t\t#i是你的iter\n",
    "        # \t\t\tloss.append(critic_loss.item())#total_loss.item()是你每一次inter输出的loss\n",
    "        # \t\t\tloss_ac.append(actor_loss)\n",
    "\n",
    "        if (done or episode_timesteps == args.episode_max_iter):\n",
    "            # +1 to account for 0 indexing. +0 on ep_timesteps since it will increment +1 even if done=True\n",
    "            print(\n",
    "                f\"Total T: {t + 1} Episode Num: {episode_num + 1} Episode T: {episode_timesteps} Reward: {episode_reward:.3f}\")\n",
    "            # Reset environment\n",
    "            state, done, info = env.reset()\n",
    "            episode_reward = 0\n",
    "            episode_timesteps = 0\n",
    "            episode_num += 1\n",
    "\n",
    "        # Evaluate episode\n",
    "        if (t + 1) % args.eval_freq == 0:\n",
    "            evaluations.append(eval_policy(policy, args, copy.deepcopy(env), args.seed))\n",
    "            np.save(f\"./results/{file_name}\", evaluations)\n",
    "            if args.save_model: policy.save(f\"./models/{file_name}\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "type(loss_ac)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss_ac"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "actorloss = []\n",
    "len(loss_ac)\n",
    "iterationac = 0\n",
    "for i in range(len(loss_ac)):\n",
    "    if i % 2 != 0:\n",
    "        actorloss.append(loss_ac[i].item())\n",
    "        iterationac += 1\n",
    "iterationac = [i for i in range(0, iterationac, 1)]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(iterationac, actorloss, label=\"loss\")\n",
    "plt.draw()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.save(f\"./results/critic_loss\", loss)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.save(f\"./results/iteration\", iteration)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(result[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 实现数据可视化中的数据平滑\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "\n",
    "def moving_average(interval, windowsize):\n",
    "    window = np.ones(int(windowsize)) / float(windowsize)\n",
    "    re = np.convolve(interval, window, 'same')\n",
    "    return re\n",
    "\n",
    "\n",
    "def LabberRing():\n",
    "    t = iterationac  #\n",
    "    print('t=', t)\n",
    "    y = actorloss  #\n",
    "    print('y=', y)\n",
    "    plt.plot(t, y, 'coral', alpha=0.15)  # plot(横坐标，纵坐标， 颜色)\n",
    "\n",
    "    y_av = moving_average(y, 200)\n",
    "    plt.plot(t, y_av, 'coral')\n",
    "    #     plt.xlabel('Time')\n",
    "    #     plt.ylabel('Value')\n",
    "    # plt.grid()网格线设置\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "\n",
    "LabberRing()  # 调用函数\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(loss_ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "actorloss = []\n",
    "len(loss_ac)\n",
    "iterationac = 0\n",
    "for i in range(len(loss_ac)):\n",
    "    if i % 2 != 0:\n",
    "        actorloss.append(loss_ac[i].item())\n",
    "        iterationac += 1\n",
    "iterationac = [i for i in range(0, iterationac, 1)]#%%\n",
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "from model.td3 import TD3\n",
    "from env.LaneChange_v2 import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import utils\n",
    "\n",
    "\n",
    "# Runs policy for X episodes and returns average reward\n",
    "# A fixed seed is used for the eval environment\n",
    "def eval_policy(policy, args, env, seed, eval_episodes=10):\n",
    "    eval_env = env\n",
    "    eval_env.seed(seed + 100)\n",
    "    episode_timesteps = 0\n",
    "\n",
    "    avg_reward = 0\n",
    "    avg_reward0 = 0\n",
    "    avg_reward1 = 0\n",
    "    avg_reward2 = 0\n",
    "    avg_reward3 = 0\n",
    "    avg_reward4 = 0\n",
    "    avg_reward5 = 0\n",
    "    num_steps = 0\n",
    "    num_bingo = 0\n",
    "    num_crash = 0\n",
    "    num_fail = 0\n",
    "    num_warn = 0\n",
    "    trajectory_x = []\n",
    "    trajectory_y = []\n",
    "\n",
    "    for i in range(eval_episodes):\n",
    "        state, done, info = eval_env.reset()\n",
    "        episode_timesteps = 0\n",
    "        # \t\teval_env.render()\n",
    "        if i == 0:\n",
    "            trajectory_x.append(eval_env.obj_veh.x)\n",
    "            trajectory_y.append(eval_env.obj_veh.y)\n",
    "        while not done and episode_timesteps != args.episode_max_iter:\n",
    "            episode_timesteps += 1\n",
    "            action = policy.select_action(np.array(state))\n",
    "            state, reward, done, info = eval_env.step(action)\n",
    "            # \t\t\teval_env.render()\n",
    "            avg_reward += reward\n",
    "            num_steps += 1\n",
    "            num_crash += info['crash']\n",
    "            num_bingo += info['bingo']\n",
    "            # num_fail += info['fail']\n",
    "            # num_warn += info['warn']\n",
    "            num_warn += info['end']\n",
    "        avg_reward0 = info['reward'][0]\n",
    "        avg_reward1 = info['reward'][1]\n",
    "        avg_reward2 = info['reward'][2]\n",
    "        avg_reward3 = info['reward'][3]\n",
    "        avg_reward4 = info['reward'][4]\n",
    "        # avg_reward5 = info['reward'][5]\n",
    "        if i == 0:\n",
    "            trajectory_x.append(eval_env.obj_veh.x)\n",
    "            trajectory_y.append(eval_env.obj_veh.y)\n",
    "\n",
    "\n",
    "avg_reward = avg_reward / eval_episodes\n",
    "avg_reward0 = avg_reward0 / eval_episodes\n",
    "avg_reward1 = avg_reward1 / eval_episodes\n",
    "avg_reward2 = avg_reward2 / eval_episodes\n",
    "avg_reward3 = avg_reward3 / eval_episodes\n",
    "avg_reward4 = avg_reward4 / eval_episodes\n",
    "avg_reward5 = avg_reward5 / eval_episodes\n",
    "num_steps = num_steps / eval_episodes\n",
    "num_crash = num_crash / eval_episodes\n",
    "num_bingo = num_bingo / eval_episodes\n",
    "num_fail = num_fail / eval_episodes\n",
    "num_warn = num_warn / eval_episodes\n",
    "\n",
    "eval_env.render(close=True)\n",
    "print(\"---------------------------------------\")\n",
    "print(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}\")\n",
    "print(\"---------------------------------------\")\n",
    "return avg_reward, num_steps, num_crash, num_bingo, num_end, avg_reward0, avg_reward1, avg_reward2, avg_reward3, avg_reward4, [\n",
    "    trajectory_x, trajectory_y]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--policy\", default=\"TD3\")  # Policy name (TD3, DDPG or OurDDPG)\n",
    "    parser.add_argument(\"--env\", default=\"LaneChangeEnv\")  # OpenAI gym environment name\n",
    "    parser.add_argument(\"--seed\", default=2, type=int)  # Sets Gym, PyTorch and Numpy seeds\n",
    "    parser.add_argument(\"--start_timesteps\", default=25e3, type=int)  # Time steps initial random policy is used\n",
    "    parser.add_argument(\"--eval_freq\", default=5e3, type=int)  # How often (time steps) we evaluate 5e3\n",
    "    parser.add_argument(\"--max_timesteps\", default=6e5, type=int)  # Max time steps to run environment\n",
    "    parser.add_argument(\"--expl_noise\", default=0.5)  # Std of Gaussian exploration noise\n",
    "    parser.add_argument(\"--batch_size\", default=256, type=int)  # Batch size for both actor and critic\n",
    "    parser.add_argument(\"--discount\", default=0.99)  # Discount factor\n",
    "    parser.add_argument(\"--tau\", default=0.005)  # Target network update rate\n",
    "    parser.add_argument(\"--policy_noise\", default=0.5)  # Noise added to target policy during critic update\n",
    "    parser.add_argument(\"--noise_clip\", default=0.5)  # Range to clip target policy noise\n",
    "    parser.add_argument(\"--policy_freq\", default=2, type=int)  # Frequency of delayed policy updates\n",
    "    parser.add_argument(\"--save_model\", action=\"store_true\")  # Save model and optimizer parameters\n",
    "    parser.add_argument(\"--load_model\", default=\"\")  # Model load file name, \"\" doesn't load, \"default\" uses file_name\n",
    "    parser.add_argument(\"--episode_max_iter\", default=500)\n",
    "    # \tparser.add_argument(\"--case_name\", default=\"CC_space30_1\")\n",
    "    parser.add_argument(\"--case_name\", default=\"CC_test\")\n",
    "\n",
    "    #     \targs = parser.parse_args()\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    file_name = f\"{args.policy}_{args.env}_{args.seed}_{args.case_name}\"\n",
    "    print(\"---------------------------------------\")\n",
    "    print(f\"Policy: {args.policy}, Env: {args.env}, Seed: {args.seed}\")\n",
    "    print(\"---------------------------------------\")\n",
    "\n",
    "    if not os.path.exists(\"./results\"):\n",
    "        os.makedirs(\"./results\")\n",
    "\n",
    "    if args.save_model and not os.path.exists(\"./models\"):\n",
    "        os.makedirs(\"./models\")\n",
    "\n",
    "    env = eval(args.env)()\n",
    "\n",
    "    # \tSet seeds\n",
    "    env.seed(args.seed)\n",
    "    env.action_space.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    max_action = float(env.action_space.high[0])\n",
    "\n",
    "    kwargs = {\n",
    "        \"state_dim\": state_dim,\n",
    "        \"action_dim\": action_dim,\n",
    "        \"max_action\": max_action,\n",
    "        \"discount\": args.discount,\n",
    "        \"tau\": args.tau,\n",
    "    }\n",
    "\n",
    "    # Initialize policy\n",
    "    if args.policy == \"TD3\":\n",
    "        # Target policy smoothing is scaled wrt the action scale\n",
    "        kwargs[\"policy_noise\"] = args.policy_noise * max_action\n",
    "        kwargs[\"noise_clip\"] = args.noise_clip * max_action\n",
    "        kwargs[\"policy_freq\"] = args.policy_freq\n",
    "        policy = TD3(**kwargs)\n",
    "    #elif args.policy == \"OurDDPG\":\n",
    "    #policy = OurDDPG.DDPG(**kwargs)\n",
    "    #elif args.policy == \"DDPG\":\n",
    "    #policy = DDPG.DDPG(**kwargs)\n",
    "\n",
    "    if args.load_model != \"\":\n",
    "        policy_file = file_name if args.load_model == \"default\" else args.load_model\n",
    "        policy.load(f\"./models/{policy_file}\")\n",
    "\n",
    "    replay_buffer = utils.ReplayBuffer(state_dim, action_dim)\n",
    "    # Evaluate untrained policy\n",
    "    evaluations = [eval_policy(policy, args, copy.deepcopy(env), args.seed)]\n",
    "\n",
    "    state, done, info = env.reset()\n",
    "    # \tenv.render()\n",
    "    episode_reward = 0\n",
    "    episode_timesteps = 0\n",
    "    episode_num = 0\n",
    "    # \tloss = []\n",
    "    # \titeration =[]\n",
    "    # \tloss_ac = []\n",
    "    # \titeration_ac=[]\n",
    "    for t in range(int(args.max_timesteps)):\n",
    "        episode_timesteps += 1\n",
    "\n",
    "        # Select action randomly or according to policy\n",
    "        if t < args.start_timesteps:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # hsr added: could add decay exploration rate to control exploration and exploitation.\n",
    "            # 2. sparse reward probelm also could add intrisic reward to encourage exploration, to  be continue.\n",
    "            action = (\n",
    "                    policy.select_action(np.array(state))\n",
    "                    + np.random.normal(0, max_action * args.expl_noise, size=action_dim)\n",
    "            ).clip(-max_action, max_action)\n",
    "\n",
    "        # Perform action\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        # \t\tenv.render()\n",
    "        done_bool = float(done) if episode_timesteps < env._max_episode_steps else 0\n",
    "\n",
    "        # Store data in replay buffer\n",
    "        replay_buffer.add(state, action, next_state, reward, done_bool)\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Train agent after collecting sufficient data\n",
    "        if t >= args.start_timesteps:\n",
    "            policy.train(replay_buffer, args.batch_size)\n",
    "        #env.render()\n",
    "\n",
    "        # \t\t\tcritic_loss = policy.train(replay_buffer, args.batch_size)\n",
    "        # \t\t\tcritic_loss,actor_loss = policy.train(replay_buffer, args.batch_size)\n",
    "        # # \t\t\tprint(actor_loss,t)\n",
    "        # \t\t\titeration.append(t)\t\t#i是你的iter\n",
    "        # \t\t\titeration_ac.append(t)\t\t#i是你的iter\n",
    "        # \t\t\tloss.append(critic_loss.item())#total_loss.item()是你每一次inter输出的loss\n",
    "        # \t\t\tloss_ac.append(actor_loss)\n",
    "\n",
    "        if (done or episode_timesteps == args.episode_max_iter):\n",
    "            # +1 to account for 0 indexing. +0 on ep_timesteps since it will increment +1 even if done=True\n",
    "            print(\n",
    "                f\"Total T: {t + 1} Episode Num: {episode_num + 1} Episode T: {episode_timesteps} Reward: {episode_reward:.3f}\")\n",
    "            # Reset environment\n",
    "            state, done, info = env.reset()\n",
    "            episode_reward = 0\n",
    "            episode_timesteps = 0\n",
    "            episode_num += 1\n",
    "\n",
    "        # Evaluate episode\n",
    "        if (t + 1) % args.eval_freq == 0:\n",
    "            evaluations.append(eval_policy(policy, args, copy.deepcopy(env), args.seed))\n",
    "            np.save(f\"./results/{file_name}\", evaluations)\n",
    "            if args.save_model: policy.save(f\"./models/{file_name}\")\n",
    "\n",
    "    # record critic loss#%%\n",
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "from model.td3 import TD3\n",
    "from env.LaneChange_v2 import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import utils\n",
    "\n",
    "\n",
    "# Runs policy for X episodes and returns average reward\n",
    "# A fixed seed is used for the eval environment\n",
    "def eval_policy(policy, args, env, seed, eval_episodes=10):\n",
    "    eval_env = env\n",
    "    eval_env.seed(seed + 100)\n",
    "    episode_timesteps = 0\n",
    "\n",
    "    avg_reward = 0\n",
    "    avg_reward0 = 0\n",
    "    avg_reward1 = 0\n",
    "    avg_reward2 = 0\n",
    "    avg_reward3 = 0\n",
    "    avg_reward4 = 0\n",
    "    avg_reward5 = 0\n",
    "    num_steps = 0\n",
    "    num_bingo = 0\n",
    "    num_crash = 0\n",
    "    num_fail = 0\n",
    "    num_warn = 0\n",
    "    trajectory_x = []\n",
    "    trajectory_y = []\n",
    "\n",
    "    for i in range(eval_episodes):\n",
    "        state, done, info = eval_env.reset()\n",
    "        episode_timesteps = 0\n",
    "        # \t\teval_env.render()\n",
    "        if i == 0:\n",
    "            trajectory_x.append(eval_env.obj_veh.x)\n",
    "            trajectory_y.append(eval_env.obj_veh.y)\n",
    "        while not done and episode_timesteps != args.episode_max_iter:\n",
    "            episode_timesteps += 1\n",
    "            action = policy.select_action(np.array(state))\n",
    "            state, reward, done, info = eval_env.step(action)\n",
    "            # \t\t\teval_env.render()\n",
    "            avg_reward += reward\n",
    "            num_steps += 1\n",
    "            num_crash += info['crash']\n",
    "            num_bingo += info['bingo']\n",
    "            num_fail += info['fail']\n",
    "            num_warn += info['warn']\n",
    "            avg_reward0 = info['reward'][0]\n",
    "            avg_reward1 = info['reward'][1]\n",
    "            avg_reward2 = info['reward'][2]\n",
    "            avg_reward3 = info['reward'][3]\n",
    "            avg_reward4 = info['reward'][4]\n",
    "            avg_reward5 = info['reward'][5]\n",
    "            if i == 0:\n",
    "                trajectory_x.append(eval_env.obj_veh.x)\n",
    "                trajectory_y.append(eval_env.obj_veh.y)\n",
    "\n",
    "    avg_reward = avg_reward / eval_episodes\n",
    "    avg_reward0 = avg_reward0 / eval_episodes\n",
    "    avg_reward1 = avg_reward1 / eval_episodes\n",
    "    avg_reward2 = avg_reward2 / eval_episodes\n",
    "    avg_reward3 = avg_reward3 / eval_episodes\n",
    "    avg_reward4 = avg_reward4 / eval_episodes\n",
    "    avg_reward5 = avg_reward5 / eval_episodes\n",
    "    num_steps = num_steps / eval_episodes\n",
    "    num_crash = num_crash / eval_episodes\n",
    "    num_bingo = num_bingo / eval_episodes\n",
    "    num_fail = num_fail / eval_episodes\n",
    "    num_warn = num_warn / eval_episodes\n",
    "\n",
    "    eval_env.render(close=True)\n",
    "    print(\"---------------------------------------\")\n",
    "    print(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}\")\n",
    "    print(\"---------------------------------------\")\n",
    "    return avg_reward, num_steps, num_crash, num_fail, num_bingo, num_warn, avg_reward0, avg_reward1, avg_reward2, avg_reward3, avg_reward4, avg_reward5, [\n",
    "        trajectory_x, trajectory_y]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--policy\", default=\"TD3\")  # Policy name (TD3, DDPG or OurDDPG)\n",
    "    parser.add_argument(\"--env\", default=\"LaneChangeEnv\")  # OpenAI gym environment name\n",
    "    parser.add_argument(\"--seed\", default=2, type=int)  # Sets Gym, PyTorch and Numpy seeds\n",
    "    parser.add_argument(\"--start_timesteps\", default=25e3, type=int)  # Time steps initial random policy is used\n",
    "    parser.add_argument(\"--eval_freq\", default=5e3, type=int)  # How often (time steps) we evaluate 5e3\n",
    "    parser.add_argument(\"--max_timesteps\", default=6e5, type=int)  # Max time steps to run environment\n",
    "    parser.add_argument(\"--expl_noise\", default=0.5)  # Std of Gaussian exploration noise\n",
    "    parser.add_argument(\"--batch_size\", default=256, type=int)  # Batch size for both actor and critic\n",
    "    parser.add_argument(\"--discount\", default=0.99)  # Discount factor\n",
    "    parser.add_argument(\"--tau\", default=0.005)  # Target network update rate\n",
    "    parser.add_argument(\"--policy_noise\", default=0.5)  # Noise added to target policy during critic update\n",
    "    parser.add_argument(\"--noise_clip\", default=0.5)  # Range to clip target policy noise\n",
    "    parser.add_argument(\"--policy_freq\", default=2, type=int)  # Frequency of delayed policy updates\n",
    "    parser.add_argument(\"--save_model\", action=\"store_true\")  # Save model and optimizer parameters\n",
    "    parser.add_argument(\"--load_model\", default=\"\")  # Model load file name, \"\" doesn't load, \"default\" uses file_name\n",
    "    parser.add_argument(\"--episode_max_iter\", default=500)\n",
    "    # \tparser.add_argument(\"--case_name\", default=\"CC_space30_1\")\n",
    "    parser.add_argument(\"--case_name\", default=\"CC_test\")\n",
    "\n",
    "    #     \targs = parser.parse_args()\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    file_name = f\"{args.policy}_{args.env}_{args.seed}_{args.case_name}\"\n",
    "    print(\"---------------------------------------\")\n",
    "    print(f\"Policy: {args.policy}, Env: {args.env}, Seed: {args.seed}\")\n",
    "    print(\"---------------------------------------\")\n",
    "\n",
    "    if not os.path.exists(\"./results\"):\n",
    "        os.makedirs(\"./results\")\n",
    "\n",
    "    if args.save_model and not os.path.exists(\"./models\"):\n",
    "        os.makedirs(\"./models\")\n",
    "\n",
    "    env = eval(args.env)()\n",
    "\n",
    "    # \tSet seeds\n",
    "    env.seed(args.seed)\n",
    "    env.action_space.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    max_action = float(env.action_space.high[0])\n",
    "\n",
    "    kwargs = {\n",
    "        \"state_dim\": state_dim,\n",
    "        \"action_dim\": action_dim,\n",
    "        \"max_action\": max_action,\n",
    "        \"discount\": args.discount,\n",
    "        \"tau\": args.tau,\n",
    "    }\n",
    "\n",
    "    # Initialize policy\n",
    "    if args.policy == \"TD3\":\n",
    "        # Target policy smoothing is scaled wrt the action scale\n",
    "        kwargs[\"policy_noise\"] = args.policy_noise * max_action\n",
    "        kwargs[\"noise_clip\"] = args.noise_clip * max_action\n",
    "        kwargs[\"policy_freq\"] = args.policy_freq\n",
    "        policy = TD3(**kwargs)\n",
    "    #elif args.policy == \"OurDDPG\":\n",
    "    #policy = OurDDPG.DDPG(**kwargs)\n",
    "    #elif args.policy == \"DDPG\":\n",
    "    #policy = DDPG.DDPG(**kwargs)\n",
    "\n",
    "    if args.load_model != \"\":\n",
    "        policy_file = file_name if args.load_model == \"default\" else args.load_model\n",
    "        policy.load(f\"./models/{policy_file}\")\n",
    "\n",
    "    replay_buffer = utils.ReplayBuffer(state_dim, action_dim)\n",
    "    # Evaluate untrained policy\n",
    "    evaluations = [eval_policy(policy, args, copy.deepcopy(env), args.seed)]\n",
    "\n",
    "    state, done, info = env.reset()\n",
    "    # \tenv.render()\n",
    "    episode_reward = 0\n",
    "    episode_timesteps = 0\n",
    "    episode_num = 0\n",
    "    # \tloss = []\n",
    "    # \titeration =[]\n",
    "    # \tloss_ac = []\n",
    "    # \titeration_ac=[]\n",
    "    for t in range(int(args.max_timesteps)):\n",
    "        episode_timesteps += 1\n",
    "\n",
    "        # Select action randomly or according to policy\n",
    "        if t < args.start_timesteps:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # hsr added: could add decay exploration rate to control exploration and exploitation.\n",
    "            # 2. sparse reward probelm also could add intrisic reward to encourage exploration, to  be continue.\n",
    "            action = (\n",
    "                    policy.select_action(np.array(state))\n",
    "                    + np.random.normal(0, max_action * args.expl_noise, size=action_dim)\n",
    "            ).clip(-max_action, max_action)\n",
    "\n",
    "        # Perform action\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        # \t\tenv.render()\n",
    "        done_bool = float(done) if episode_timesteps < env._max_episode_steps else 0\n",
    "\n",
    "        # Store data in replay buffer\n",
    "        replay_buffer.add(state, action, next_state, reward, done_bool)\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Train agent after collecting sufficient data\n",
    "        if t >= args.start_timesteps:\n",
    "            policy.train(replay_buffer, args.batch_size)\n",
    "        #env.render()\n",
    "\n",
    "        # \t\t\tcritic_loss = policy.train(replay_buffer, args.batch_size)\n",
    "        # \t\t\tcritic_loss,actor_loss = policy.train(replay_buffer, args.batch_size)\n",
    "        # # \t\t\tprint(actor_loss,t)\n",
    "        # \t\t\titeration.append(t)\t\t#i是你的iter\n",
    "        # \t\t\titeration_ac.append(t)\t\t#i是你的iter\n",
    "        # \t\t\tloss.append(critic_loss.item())#total_loss.item()是你每一次inter输出的loss\n",
    "        # \t\t\tloss_ac.append(actor_loss)\n",
    "\n",
    "        if (done or episode_timesteps == args.episode_max_iter):\n",
    "            # +1 to account for 0 indexing. +0 on ep_timesteps since it will increment +1 even if done=True\n",
    "            print(\n",
    "                f\"Total T: {t + 1} Episode Num: {episode_num + 1} Episode T: {episode_timesteps} Reward: {episode_reward:.3f}\")\n",
    "            # Reset environment\n",
    "            state, done, info = env.reset()\n",
    "            episode_reward = 0\n",
    "            episode_timesteps = 0\n",
    "            episode_num += 1\n",
    "\n",
    "        # Evaluate episode\n",
    "        if (t + 1) % args.eval_freq == 0:\n",
    "            evaluations.append(eval_policy(policy, args, copy.deepcopy(env), args.seed))\n",
    "            np.save(f\"./results/{file_name}\", evaluations)\n",
    "            if args.save_model: policy.save(f\"./models/{file_name}\")\n",
    "\n",
    "    # record critic loss\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "type(loss_ac)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss_ac"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "actorloss = []\n",
    "len(loss_ac)\n",
    "iterationac = 0\n",
    "for i in range(len(loss_ac)):\n",
    "    if i % 2 != 0:\n",
    "        actorloss.append(loss_ac[i].item())\n",
    "        iterationac += 1\n",
    "iterationac = [i for i in range(0, iterationac, 1)]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(iterationac, actorloss, label=\"loss\")\n",
    "plt.draw()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.save(f\"./results/critic_loss\", loss)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.save(f\"./results/iteration\", iteration)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(result[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 实现数据可视化中的数据平滑\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "\n",
    "def moving_average(interval, windowsize):\n",
    "    window = np.ones(int(windowsize)) / float(windowsize)\n",
    "    re = np.convolve(interval, window, 'same')\n",
    "    return re\n",
    "\n",
    "\n",
    "def LabberRing():\n",
    "    t = iterationac  #\n",
    "    print('t=', t)\n",
    "    y = actorloss  #\n",
    "    print('y=', y)\n",
    "    plt.plot(t, y, 'coral', alpha=0.15)  # plot(横坐标，纵坐标， 颜色)\n",
    "\n",
    "    y_av = moving_average(y, 200)\n",
    "    plt.plot(t, y_av, 'coral')\n",
    "    #     plt.xlabel('Time')\n",
    "    #     plt.ylabel('Value')\n",
    "    # plt.grid()网格线设置\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "\n",
    "LabberRing()  # 调用函数\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "type(loss_ac)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss_ac"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "actorloss = []\n",
    "len(loss_ac)\n",
    "iterationac = 0\n",
    "for i in range(len(loss_ac)):\n",
    "    if i % 2 != 0:\n",
    "        actorloss.append(loss_ac[i].item())\n",
    "        iterationac += 1\n",
    "iterationac = [i for i in range(0, iterationac, 1)]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(iterationac, actorloss, label=\"loss\")\n",
    "plt.draw()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.save(f\"./results/critic_loss\", loss)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.save(f\"./results/iteration\", iteration)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(result[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 实现数据可视化中的数据平滑\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "def moving_average(interval, windowsize):\n",
    "    window = np.ones(int(windowsize)) / float(windowsize)\n",
    "    re = np.convolve(interval, window, 'same')\n",
    "    return re\n",
    "\n",
    "def LabberRing():\n",
    "    t = iterationac  #\n",
    "    print('t=', t)\n",
    "    y = actorloss   #\n",
    "    print('y=', y)\n",
    "    plt.plot(t, y, 'coral', alpha=0.15)     # plot(横坐标，纵坐标， 颜色)\n",
    "\n",
    "    y_av = moving_average(y, 200)\n",
    "    plt.plot(t, y_av, 'coral')\n",
    "#     plt.xlabel('Time')\n",
    "#     plt.ylabel('Value')\n",
    "    # plt.grid()网格线设置\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "LabberRing()  # 调用函数\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(iterationac, actorloss, label=\"loss\")\n",
    "plt.draw()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.save(f\"./results/critic_loss\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f\"./results/iteration\", iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现数据可视化中的数据平滑\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    " \n",
    "def moving_average(interval, windowsize):\n",
    "    window = np.ones(int(windowsize)) / float(windowsize)\n",
    "    re = np.convolve(interval, window, 'same')\n",
    "    return re\n",
    " \n",
    "def LabberRing():\n",
    "    t = iterationac  # \n",
    "    print('t=', t)\n",
    "    y = actorloss   # \n",
    "    print('y=', y)\n",
    "    plt.plot(t, y, 'coral', alpha=0.15)     # plot(横坐标，纵坐标， 颜色)\n",
    "    \n",
    "    y_av = moving_average(y, 200)\n",
    "    plt.plot(t, y_av, 'coral')\n",
    "#     plt.xlabel('Time')\n",
    "#     plt.ylabel('Value')\n",
    "    # plt.grid()网格线设置\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    return\n",
    " \n",
    "LabberRing()  # 调用函数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Attachments",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
